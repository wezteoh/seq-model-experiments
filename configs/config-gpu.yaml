# Training run config
train:
  seed: 42
  validate_at_start: true
  test: true
  ckpt: null
  ckpt_dir: ~/ckpt

# Data config (MNIST sequence task)
dataset:
  dataset: mnist
  bsz: 128
  num_workers: 0
  data_dir: ~/data

# Small model for speed (used by CPU class after the code tweak below)
model:
  args:
    d_model: 64
    n_layer: 4
    d_state: 128
    d_conv: 4
    expand: 4
    headdim: 16
    chunk_size: 16
    vocab_size: 256
    pad_vocab_size_multiple: 16
    embedding: false

# Optimizer
lr: 0.001
weight_decay: 0.01
lr_schedule: true
pct_start: 0.1
total_steps: null
num_samples_while_training: 4
sample_prefix_length: 256
task_type: generation
use_fused_kernel: false

# Lightning Trainer
trainer:
  accelerator: gpu
  devices: 1
  precision: 32
  max_epochs: 100
  fast_dev_run: false          # one batch train/val/test and exit
  limit_train_batches: null       # safety; only a couple batches
  limit_val_batches: null
  limit_test_batches: null
  log_every_n_steps: null
  enable_checkpointing: true

wandb:
  project: mamba2-mnist
  name: torchmodel
callbacks: {}