# Training run config
train:
  seed: 42
  validate_at_start: true
  test: true
  ckpt: null
  ckpt_dir: ~/ckpt/mamba2attn-xkparams-schedule

dataset:
  name: mnist_generation
  bsz: 128
  num_workers: 0
  data_dir: ~/data
  input_type: raw
  target_type: token

model:
  name: mamba2_fused
  args:
    n_layer: 8
    d_model: 64
    d_intermediate: 0
    vocab_size: 256
    attn_layer_idx: [2,5]
    attn_cfg:
      num_heads: 4
      mlp_dim: 0
      qkv_proj_bias: false
      out_proj_bias: false
      softmax_scale: null
      causal: true
      d_conv: 0
      rotary_emb_dim: 16
      rotary_emb_base: 10000.0
      rotary_emb_interleaved: true
    ssm_cfg:
      layer: Mamba2
      d_state: 128
      d_conv: 4
      expand: 1
      headdim: 16
      ngroups: 1
      chunk_size: 128
    rms_norm: true
    fused_add_norm: true
    residual_in_fp32: false
    input_type: raw
    output_type: logits

metrics: ["loss", "accuracy"]

# Optimizer
lr: 0.003
weight_decay: 0.01
lr_schedule: true
pct_start: 0.1
div_factor: 15.0
final_div_factor: 500.0
beta1: 0.9
beta2: 0.95

total_steps: null
task_type: generation
loss: cross_entropy
output2input_preprocess_fn_name: mnist_token2raw

# Lightning Trainer
trainer:
  accelerator: gpu
  devices: 1
  precision: "bf16-true"
  max_epochs: 100
  fast_dev_run: false          # one batch train/val/test and exit
  limit_train_batches: null       # safety; only a couple batches
  limit_val_batches: null
  limit_test_batches: null
  log_every_n_steps: null
  enable_checkpointing: true
  gradient_clip_val: 1.0
  gradient_clip_algorithm: norm

wandb:
  project: mamba2attn-mnist
  name: newxkparams-schedule-lr3e-3
callbacks:
  ImagePrefixSamplerCallback:
    num_samples: 8
    every_n_epochs: 1
    sample_prefix_length: 257
    max_length: 785